// AI Service - Integra√ß√£o h√≠brida com IA para respostas inteligentes
// VERSION: v2.4.1 | DATE: 2025-01-27 | AUTHOR: Lucas Gravina - VeloHub Development Team
const { OpenAI } = require('openai');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const config = require('../../config');

class AIService {
  constructor() {
    this.openai = null;
    this.gemini = null;
    this.openaiModel = "gpt-4o-mini"; // Modelo otimizado para custo (fallback)
    this.geminiModel = "gemini-2.5-pro"; // Modelo Gemini 2.5 Pro (prim√°rio)
  }

  /**
   * Inicializa o cliente OpenAI apenas quando necess√°rio
   */
  _initializeOpenAI() {
    if (!this.openai && this.isOpenAIConfigured()) {
      this.openai = new OpenAI({
        apiKey: config.OPENAI_API_KEY,
      });
    }
    return this.openai;
  }

  /**
   * Inicializa o cliente Gemini apenas quando necess√°rio
   */
  _initializeGemini() {
    if (!this.gemini && this.isGeminiConfigured()) {
      this.gemini = new GoogleGenerativeAI(config.GEMINI_API_KEY);
    }
    return this.gemini;
  }

  /**
   * Gera resposta inteligente baseada na pergunta e contexto
   * FLUXO: Gemini (prim√°rio) ‚Üí OpenAI (fallback) ‚Üí Resposta padr√£o
   * @param {string} question - Pergunta do usu√°rio
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @param {string} userId - ID do usu√°rio
   * @param {string} email - Email do usu√°rio
   * @param {Object} searchResults - Resultados da busca h√≠brida (opcional)
   * @param {string} formatType - Tipo de formata√ß√£o (conversational, whatsapp, email)
   * @returns {Promise<Object>} Resposta com provider usado
   */
  async generateResponse(question, context = "", sessionHistory = [], userId = null, email = null, searchResults = null, formatType = 'conversational') {
    try {
      // 1. TENTAR GEMINI PRIMEIRO (IA PRIM√ÅRIA)
      if (this.isGeminiConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Tentando Gemini (prim√°rio) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithGemini(question, context, sessionHistory, userId, email, searchResults, formatType);
          return {
            response: response,
            provider: 'Gemini',
            model: this.geminiModel,
            success: true
          };
        } catch (geminiError) {
          console.warn('‚ö†Ô∏è AI Service: Gemini falhou, tentando OpenAI fallback:', geminiError.message);
        }
      }

      // 2. FALLBACK PARA OPENAI (IA SECUND√ÅRIA)
      if (this.isOpenAIConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Usando OpenAI (fallback) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithOpenAI(question, context, sessionHistory, userId, email, searchResults, formatType);
          return {
            response: response,
            provider: 'OpenAI',
            model: this.openaiModel,
            success: true
          };
        } catch (openaiError) {
          console.error('‚ùå AI Service: OpenAI tamb√©m falhou:', openaiError.message);
        }
      }

      // 3. SE AMBOS FALHARAM
      throw new Error('Nenhuma API de IA dispon√≠vel');
      
    } catch (error) {
      console.error('‚ùå AI Service Error:', error.message);
      
      // 4. FALLBACK PARA RESPOSTA PADR√ÉO
      return {
        response: `Desculpe, n√£o consegui processar sua pergunta no momento. 
        Por favor, tente novamente ou entre em contato com nosso suporte.`,
        provider: 'Fallback',
        model: 'none',
        success: false,
        error: error.message
      };
    }
  }

  /**
   * Obt√©m a persona padr√£o do VeloBot
   * @returns {string} Persona formatada
   */
  getPersona() {
    return `# VELOBOT - ASSISTENTE OFICIAL VELOTAX

## IDENTIDADE
- Nome: VeloBot
- Empresa: Velotax
- Fun√ß√£o: Assistente de atendimento ao cliente
- Tom: Profissional, direto, prestativo, conversacional, solid√°rio.

## COMPORTAMENTO
- Responda APENAS com a informa√ß√£o solicitada
- Seja direto, sem pre√¢mbulos ou confirma√ß√µes
- Use portugu√™s brasileiro claro e objetivo
- As intera√ß√µes esperadas s√£o de chunho textual, sem adicionar informa√ß√µes gen√©ricas, criadas, ou realizar pesquisas externas.
- Apenas os conhecimentos fornecidos s√£o v√°lidos. N√£o invente informa√ß√µes.
- N√ÉO use conhecimento externo ou associa√ß√µes que n√£o estejam nos dados fornecidos.
- Se a resposta contiver muitos termos t√©cnicos, simplifique para um n√≠vel de f√°cil compreens√£o.
- Se n√£o souber, diga: "N√£o encontrei essa informa√ß√£o na base de conhecimento dispon√≠vel"

## FONTES DE INFORMA√á√ÉO
- Base de dados: Bot_perguntas (MongoDB)
- Artigos: Documenta√ß√£o interna
- Prioridade: Informa√ß√£o s√≥lida > IA generativa

## FORMATO DE RESPOSTA
- Direto ao ponto
- Sem "Entendi", "Compreendo", etc.
- M√°ximo 200 palavras
- Foco na solu√ß√£o pr√°tica`;
  }

  /**
   * Obt√©m a persona para formata√ß√£o WhatsApp
   * @returns {string} Persona formatada para WhatsApp
   */
  getWhatsAppPersona() {
    return `# VELOBOT - REFORMULADOR WHATSAPP

## IDENTIDADE
- Nome: VeloBot
- Empresa: Velotax
- Fun√ß√£o: Reformulador de respostas para WhatsApp
- Tom: Informal, amig√°vel, direto, com emojis

## COMPORTAMENTO
- Reformule a resposta para o formato WhatsApp
- Use linguagem informal e amig√°vel
- Seja conciso e direto
- Use quebras de linha para facilitar leitura
- Evite jarg√µes t√©cnicos complexos
- Use abrevia√ß√µes comuns do WhatsApp quando apropriado

## FORMATO WHATSAPP
- M√°ximo 150 palavras
- Quebras de linha frequentes
- Emojis estrat√©gicos
- Linguagem coloquial
- Foco na praticidade

## ESTRUTURA
- Sauda√ß√£o informal (se apropriado)
- Informa√ß√£o principal
- Detalhes importantes
- Encerramento amig√°vel`;
  }

  /**
   * Obt√©m a persona para formata√ß√£o E-mail formal
   * @returns {string} Persona formatada para E-mail
   */
  getEmailPersona() {
    return `# VELOBOT - REFORMULADOR E-MAIL FORMAL

## IDENTIDADE
- Nome: VeloBot
- Empresa: Velotax
- Fun√ß√£o: Reformulador de respostas para E-mail formal
- Tom: Profissional, formal, estruturado, cort√™s

## COMPORTAMENTO
- Reformule a resposta para o formato de e-mail formal
- Use linguagem profissional e cort√™s
- Estruture a informa√ß√£o de forma clara e organizada
- Use t√≠tulos e subt√≠tulos quando apropriado
- Seja detalhado mas objetivo
- Mantenha tom respeitoso e profissional

## FORMATO E-MAIL FORMAL
- M√°ximo 300 palavras
- Estrutura clara com t√≠tulos
- Linguagem formal e cort√™s
- Detalhamento apropriado
- Foco na completude da informa√ß√£o

## ESTRUTURA
- Sauda√ß√£o formal
- Assunto/contexto
- Informa√ß√£o principal estruturada
- Detalhes relevantes
- Encerramento cort√™s
- Assinatura da empresa`;
  }

  /**
   * Obt√©m a persona baseada no tipo de formata√ß√£o
   * @param {string} formatType - Tipo de formata√ß√£o (conversational, whatsapp, email)
   * @returns {string} Persona apropriada
   */
  _getPersonaByFormat(formatType) {
    console.log(`üîç Persona Debug: formatType recebido: "${formatType}"`);
    
    switch (formatType) {
      case 'whatsapp':
        console.log('üîç Persona Debug: Selecionando persona WhatsApp');
        return this.getWhatsAppPersona();
      case 'email':
        console.log('üîç Persona Debug: Selecionando persona E-mail');
        return this.getEmailPersona();
      case 'conversational':
      default:
        console.log('üîç Persona Debug: Selecionando persona conversacional (padr√£o)');
        return this.getPersona();
    }
  }

  /**
   * Analisa pergunta do usu√°rio contra base de dados usando IA
   * @param {string} question - Pergunta do usu√°rio
   * @param {Array} botPerguntasData - Dados do MongoDB Bot_perguntas
   * @returns {Promise<Object>} An√°lise da IA com op√ß√µes relevantes
   */
  async analyzeQuestionWithAI(question, botPerguntasData) {
    try {
      console.log(`ü§ñ AI Analyzer: Analisando pergunta: "${question}"`);
      
      if (!this.isGeminiConfigured()) {
        throw new Error('IA n√£o configurada para an√°lise');
      }

      // Construir contexto com todas as perguntas da base
      const contextData = botPerguntasData.map((item, index) => {
        return `${index + 1}. **Pergunta:** ${item.Pergunta || item.pergunta || 'N/A'}
   **Palavras-chave:** ${item["Palavras-chave"] || item.palavras_chave || 'N/A'}
   **Sin√¥nimos:** ${item.Sinonimos || item.sinonimos || 'N/A'}
   **Resposta:** ${(item.Resposta || item.resposta || '').substring(0, 100)}...`;
      }).join('\n\n');

      const analysisPrompt = `# ANALISADOR DE PERGUNTAS - VELOBOT

## REGRAS CR√çTICAS - LEIA COM ATEN√á√ÉO
- Use APENAS as informa√ß√µes fornecidas na base de dados
- N√ÉO fa√ßa associa√ß√µes externas ou use conhecimento pr√≥prio
- N√ÉO invente conex√µes que n√£o existem nos dados
- Se n√£o houver match claro, retorne vazio

## PERGUNTA DO USU√ÅRIO
"${question}"

## BASE DE DADOS DISPON√çVEL
${contextData}

## INSTRU√á√ïES
1. Compare APENAS com as perguntas, palavras-chave e sin√¥nimos fornecidos
2. Identifique APENAS matches diretos e √≥bvios
3. Se n√£o houver match claro, retorne vazio
4. Retorne APENAS os n√∫meros das op√ß√µes com match direto

## FORMATO DE RESPOSTA
Responda APENAS com os n√∫meros das op√ß√µes com match direto, separados por v√≠rgula.
Se n√£o houver match, responda: NENHUM

## CRIT√âRIOS DE RELEV√ÇNCIA (RESTRITIVOS)
- Match exato na pergunta
- Palavras-chave id√™nticas
- Sin√¥nimos exatos fornecidos
- N√ÉO use conhecimento externo

## RESPOSTA:`;

      const gemini = this._initializeGemini();
      const model = gemini.getGenerativeModel({ model: this.geminiModel });
      
      const result = await model.generateContent(analysisPrompt);
      const response = result.response.text().trim();
      
      console.log(`ü§ñ AI Analyzer: Resposta da IA: "${response}"`);
      console.log(`üîç AI Analyzer: Tamanho da resposta: ${response.length} caracteres`);
      
      // Verificar se a IA retornou "NENHUM" (sem matches)
      if (response.toUpperCase().includes('NENHUM') || response.trim() === '') {
        console.log('‚ùå AI Analyzer: IA retornou NENHUM - nenhuma op√ß√£o relevante identificada');
        return { relevantOptions: [], needsClarification: false };
      }

      // Extrair n√∫meros da resposta
      const relevantIndices = response.match(/\d+/g);
      if (!relevantIndices || relevantIndices.length === 0) {
        console.log('‚ùå AI Analyzer: Nenhuma op√ß√£o relevante identificada');
        return { relevantOptions: [], needsClarification: false };
      }

      // Converter para √≠ndices reais (subtrair 1)
      const indices = relevantIndices.map(num => parseInt(num) - 1).filter(idx => idx >= 0 && idx < botPerguntasData.length);
      
      console.log(`‚úÖ AI Analyzer: ${indices.length} op√ß√µes relevantes identificadas: ${indices.join(', ')}`);
      
      // Se apenas 1 op√ß√£o relevante, n√£o precisa de esclarecimento
      if (indices.length === 1) {
        return {
          relevantOptions: [botPerguntasData[indices[0]]],
          needsClarification: false,
          bestMatch: botPerguntasData[indices[0]]
        };
      }
      
      // M√∫ltiplas op√ß√µes = precisa de esclarecimento
      const relevantOptions = indices.map(idx => botPerguntasData[idx]);
      
      return {
        relevantOptions: relevantOptions,
        needsClarification: true,
        bestMatch: null
      };

    } catch (error) {
      console.error('‚ùå AI Analyzer Error:', error.message);
      return { relevantOptions: [], needsClarification: false, error: error.message };
    }
  }

  /**
   * Gera resposta usando Gemini (IA PRIM√ÅRIA)
   */
  async _generateWithGemini(question, context, sessionHistory, userId, email, searchResults = null, formatType = 'conversational') {
    console.log(`üîç Gemini Debug: formatType recebido: "${formatType}"`);
    
    const gemini = this._initializeGemini();
    if (!gemini) {
      throw new Error('Falha ao inicializar cliente Gemini');
    }

    const model = gemini.getGenerativeModel({ model: this.geminiModel });
    
    // Construir prompt completo (system + user) otimizado para Gemini
    const systemPrompt = this._getPersonaByFormat(formatType);
    console.log(`üîç Gemini Debug: Persona selecionada para formatType "${formatType}"`);

    const userPrompt = this.buildOptimizedPrompt(question, context, sessionHistory, searchResults);
    
    // Combinar system + user prompt para Gemini
    const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;
    
    console.log(`ü§ñ Gemini: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const result = await model.generateContent(fullPrompt);
    const response = result.response.text();
    
    console.log(`‚úÖ Gemini: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Gera resposta usando OpenAI (IA FALLBACK)
   */
  async _generateWithOpenAI(question, context, sessionHistory, userId, email, searchResults = null, formatType = 'conversational') {
    const openai = this._initializeOpenAI();
    if (!openai) {
      throw new Error('Falha ao inicializar cliente OpenAI');
    }

    // Construir prompt otimizado (baseado no chatbot Vercel)
    const prompt = this.buildOptimizedPrompt(question, context, sessionHistory, searchResults);
    
    console.log(`ü§ñ OpenAI: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const completion = await openai.chat.completions.create({
      model: this.openaiModel,
      messages: [
        {
          role: "system",
          content: this._getPersonaByFormat(formatType)
        },
        {
          role: "user",
          content: prompt
        }
      ],
      temperature: 0.1, // Mais determin√≠stico
      max_tokens: 512, // Respostas mais concisas
      top_p: 0.8,
      frequency_penalty: 0.1,
      presence_penalty: 0.1
    });

    const response = completion.choices[0].message.content;
    
    console.log(`‚úÖ OpenAI: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Constr√≥i contexto estruturado com informa√ß√µes organizadas
   * @param {Object} searchResults - Resultados da busca h√≠brida
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Contexto estruturado
   */
  buildStructuredContext(searchResults, sessionHistory) {
    let context = `## CONTEXTO DA CONSULTA\n\n`;
    
    // Informa√ß√£o principal (Bot_perguntas)
    if (searchResults && searchResults.botPergunta) {
      context += `### INFORMA√á√ÉO PRINCIPAL
**Pergunta:** ${searchResults.botPergunta.Pergunta || searchResults.botPergunta.pergunta}
**Resposta:** ${searchResults.botPergunta.Resposta || searchResults.botPergunta.resposta}
**Relev√¢ncia:** ${searchResults.botPergunta.relevanceScore || 'N/A'}/10
**Fonte:** Bot_perguntas (MongoDB)

`;
    }
    
    // Artigos relacionados
    if (searchResults && searchResults.articles && searchResults.articles.length > 0) {
      context += `### ARTIGOS RELACIONADOS\n`;
      searchResults.articles.forEach((article, index) => {
        context += `${index + 1}. **${article.title}**
   - Relev√¢ncia: ${article.relevanceScore || 'N/A'}/10
   - Conte√∫do: ${article.content.substring(0, 150)}...
   
`;
      });
    }
    
    // Hist√≥rico da sess√£o
    if (sessionHistory && sessionHistory.length > 0) {
      context += `### HIST√ìRICO DA CONVERSA\n`;
      sessionHistory.slice(-3).forEach(h => {
        context += `- ${h.role}: ${h.content}\n`;
      });
      context += `\n`;
    }
    
    return context;
  }

  /**
   * Constr√≥i o prompt otimizado com contexto e hist√≥rico
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento (legado)
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @param {Object} searchResults - Resultados da busca h√≠brida (novo)
   * @returns {string} Prompt formatado
   */
  buildOptimizedPrompt(question, context, sessionHistory, searchResults = null) {
    // Usar contexto estruturado se searchResults estiver dispon√≠vel
    const structuredContext = searchResults ? 
      this.buildStructuredContext(searchResults, sessionHistory) : 
      `### CONTEXTO DA BASE DE CONHECIMENTO
${context || "Nenhum contexto espec√≠fico encontrado."}

### HIST√ìRICO DE CONVERSA
${sessionHistory.length > 0 ? 
  sessionHistory.map(h => `${h.role}: ${h.content}`).join("\n") : 
  'Primeira pergunta da sess√£o.'}`;

    return `${structuredContext}
## PERGUNTA ATUAL
**Usu√°rio:** ${question}

## INSTRU√á√ïES
- Use APENAS as informa√ß√µes do contexto acima
- Se a pergunta for sobre cr√©dito, foco em prazos e processos
- Se for sobre documentos, liste exatamente o que √© necess√°rio
- Se for sobre prazos, seja espec√≠fico com datas e tempos
- Se n√£o houver informa√ß√£o suficiente, diga claramente

## RESPOSTA:`;
  }

  /**
   * Valida a qualidade da resposta gerada pela IA
   * @param {string} response - Resposta da IA
   * @param {string} question - Pergunta original
   * @returns {Object} Resultado da valida√ß√£o
   */
  validateResponse(response, question) {
    // Verificar se a resposta √© muito gen√©rica
    const genericResponses = [
      "n√£o encontrei essa informa√ß√£o",
      "n√£o tenho essa informa√ß√£o",
      "n√£o posso ajudar com isso",
      "n√£o sei",
      "n√£o consigo"
    ];
    
    const isGeneric = genericResponses.some(generic => 
      response.toLowerCase().includes(generic)
    );
    
    if (isGeneric && response.length < 50) {
      return {
        valid: false,
        reason: "Resposta muito gen√©rica",
        suggestion: "Buscar em outras fontes ou reformular pergunta"
      };
    }
    
    // Verificar se a resposta √© muito longa
    if (response.length > 500) {
      return {
        valid: false,
        reason: "Resposta muito longa",
        suggestion: "Resumir para m√°ximo 200 palavras"
      };
    }
    
    return { valid: true };
  }

  /**
   * Constr√≥i o prompt com contexto e hist√≥rico (m√©todo legado)
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt formatado
   */
  buildPrompt(question, context, sessionHistory) {
    return this.buildOptimizedPrompt(question, context, sessionHistory);
  }

  /**
   * Verifica se a API OpenAI est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isOpenAIConfigured() {
    return !!config.OPENAI_API_KEY && config.OPENAI_API_KEY !== 'your_openai_api_key_here';
  }

  /**
   * Verifica se a API Gemini est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isGeminiConfigured() {
    return !!config.GEMINI_API_KEY && config.GEMINI_API_KEY !== 'your_gemini_api_key_here';
  }

  /**
   * Verifica se alguma API est√° configurada
   * @returns {boolean} Status da configura√ß√£o
   */
  isConfigured() {
    return this.isOpenAIConfigured() || this.isGeminiConfigured();
  }

  /**
   * Obt√©m status das configura√ß√µes de IA
   * @returns {Object} Status das configura√ß√µes
   */
  getConfigurationStatus() {
    return {
      gemini: {
        configured: this.isGeminiConfigured(),
        model: this.geminiModel,
        priority: 'primary'
      },
      openai: {
        configured: this.isOpenAIConfigured(),
        model: this.openaiModel,
        priority: 'fallback'
      },
      anyAvailable: this.isConfigured()
    };
  }

  /**
   * Testa a conex√£o com as APIs de IA
   * @returns {Promise<Object>} Status das conex√µes
   */
  async testConnection() {
    const results = {
      gemini: { available: false, model: this.geminiModel, priority: 'primary' },
      openai: { available: false, model: this.openaiModel, priority: 'fallback' },
      anyAvailable: false
    };

    // Teste Gemini (prim√°rio)
    if (this.isGeminiConfigured()) {
      try {
        const gemini = this._initializeGemini();
        if (gemini) {
          const model = gemini.getGenerativeModel({ model: this.geminiModel });
          const result = await model.generateContent("Teste de conex√£o");
          results.gemini.available = true;
          console.log('‚úÖ Gemini: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå Gemini: Erro no teste de conex√£o:', error.message);
      }
    }

    // Teste OpenAI (fallback)
    if (this.isOpenAIConfigured()) {
      try {
        const openai = this._initializeOpenAI();
        if (openai) {
          const completion = await openai.chat.completions.create({
            model: this.openaiModel,
            messages: [{ role: "user", content: "Teste de conex√£o" }],
            max_tokens: 10,
          });
          results.openai.available = true;
          console.log('‚úÖ OpenAI: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå OpenAI: Erro no teste de conex√£o:', error.message);
      }
    }

    results.anyAvailable = results.gemini.available || results.openai.available;
    
    if (!results.anyAvailable) {
      console.warn('‚ö†Ô∏è Nenhuma API de IA dispon√≠vel');
    }

    return results;
  }
}

module.exports = new AIService();
