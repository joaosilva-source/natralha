// OpenAI Service - Integra√ß√£o com IA para respostas inteligentes
// VERSION: v2.0.1 | DATE: 2025-01-27 | AUTHOR: Lucas Gravina - VeloHub Development Team
const { OpenAI } = require('openai');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const config = require('../../config');

class OpenAIService {
  constructor() {
    this.openai = null;
    this.gemini = null;
    this.openaiModel = "gpt-4o-mini"; // Modelo otimizado para custo
    this.geminiModel = "gemini-2.5-pro"; // Modelo Gemini 2.5 Pro
  }

  /**
   * Inicializa o cliente OpenAI apenas quando necess√°rio
   */
  _initializeOpenAI() {
    if (!this.openai && this.isOpenAIConfigured()) {
      this.openai = new OpenAI({
        apiKey: config.OPENAI_API_KEY,
      });
    }
    return this.openai;
  }

  /**
   * Inicializa o cliente Gemini apenas quando necess√°rio
   */
  _initializeGemini() {
    if (!this.gemini && this.isGeminiConfigured()) {
      this.gemini = new GoogleGenerativeAI(config.GEMINI_API_KEY);
    }
    return this.gemini;
  }

  /**
   * Gera resposta inteligente baseada na pergunta e contexto
   * @param {string} question - Pergunta do usu√°rio
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @param {string} userId - ID do usu√°rio
   * @param {string} email - Email do usu√°rio
   * @returns {Promise<string>} Resposta gerada pela IA
   */
  async generateResponse(question, context = "", sessionHistory = [], userId = null, email = null) {
    try {
      // Tentar Gemini primeiro (prim√°rio)
      if (this.isGeminiConfigured()) {
        try {
          return await this._generateWithGemini(question, context, sessionHistory, userId, email);
        } catch (geminiError) {
          console.warn('‚ö†Ô∏è Gemini falhou, tentando OpenAI:', geminiError.message);
        }
      }

      // Fallback para OpenAI
      if (this.isOpenAIConfigured()) {
        try {
          return await this._generateWithOpenAI(question, context, sessionHistory, userId, email);
        } catch (openaiError) {
          console.error('‚ùå OpenAI tamb√©m falhou:', openaiError.message);
        }
      }

      // Se ambos falharam
      throw new Error('Nenhuma API de IA dispon√≠vel');
      
    } catch (error) {
      console.error('‚ùå AI Service Error:', error.message);
      
      // Fallback para resposta padr√£o
      return `Desculpe, n√£o consegui processar sua pergunta no momento. 
      Por favor, tente novamente ou entre em contato com nosso suporte.`;
    }
  }

  /**
   * Gera resposta usando OpenAI
   */
  async _generateWithOpenAI(question, context, sessionHistory, userId, email) {
    const openai = this._initializeOpenAI();
    if (!openai) {
      throw new Error('Falha ao inicializar cliente OpenAI');
    }

    // Construir prompt otimizado (baseado no chatbot Vercel)
    const prompt = this.buildOptimizedPrompt(question, context, sessionHistory);
    
    console.log(`ü§ñ OpenAI: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const completion = await openai.chat.completions.create({
      model: this.openaiModel,
      messages: [
        {
          role: "system",
          content: `### PERSONA
Voc√™ √© o VeloBot, assistente oficial da Velotax. Responda com base no hist√≥rico de conversa, no contexto da planilha e nos sites autorizados.

### REGRAS
- Se a nova pergunta for amb√≠gua, use o hist√≥rico para entender o que o atendente quis dizer.
- Seja direto e claro, mas natural.
- Se o atendente disser "n√£o entendi", reformule sua √∫ltima resposta de forma mais simples.
- Se n√£o encontrar no contexto ou nos sites, diga: "N√£o encontrei essa informa√ß√£o nem na base de conhecimento nem nos sites oficiais."
- Sempre responda em portugu√™s do Brasil.`
        },
        {
          role: "user",
          content: prompt
        }
      ],
      temperature: 0.2,
      max_tokens: 1024,
    });

    const response = completion.choices[0].message.content;
    
    console.log(`‚úÖ OpenAI: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Gera resposta usando Gemini (Fallback)
   */
  async _generateWithGemini(question, context, sessionHistory, userId, email) {
    const gemini = this._initializeGemini();
    if (!gemini) {
      throw new Error('Falha ao inicializar cliente Gemini');
    }

    const model = gemini.getGenerativeModel({ model: this.geminiModel });
    
    // Construir prompt completo (system + user) igual ao OpenAI
    const systemPrompt = `### PERSONA
Voc√™ √© o VeloBot, assistente oficial da Velotax. Responda com base no hist√≥rico de conversa, no contexto da planilha e nos sites autorizados.

### REGRAS
- Se a nova pergunta for amb√≠gua, use o hist√≥rico para entender o que o atendente quis dizer.
- Seja direto e claro, mas natural.
- Se o atendente disser "n√£o entendi", reformule sua √∫ltima resposta de forma mais simples.
- Se n√£o encontrar no contexto ou nos sites, diga: "N√£o encontrei essa informa√ß√£o nem na base de conhecimento nem nos sites oficiais."
- Sempre responda em portugu√™s do Brasil.`;

    const userPrompt = this.buildOptimizedPrompt(question, context, sessionHistory);
    
    // Combinar system + user prompt para Gemini
    const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;
    
    console.log(`ü§ñ Gemini: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const result = await model.generateContent(fullPrompt);
    const response = result.response.text();
    
    console.log(`‚úÖ Gemini: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Constr√≥i o prompt otimizado com contexto e hist√≥rico (baseado no chatbot Vercel)
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt formatado
   */
  buildOptimizedPrompt(question, context, sessionHistory) {
    let prompt = `
### HIST√ìRICO DE CONVERSA
${sessionHistory.length > 0 ? 
  sessionHistory.map(h => `${h.role}: ${h.content}`).join("\n") : 
  'Primeira pergunta da sess√£o.'}

### CONTEXTO DA PLANILHA
${context || "Nenhum contexto espec√≠fico encontrado."}

### PERGUNTA ATUAL
"${question}"
`;

    return prompt;
  }

  /**
   * Constr√≥i o prompt com contexto e hist√≥rico (m√©todo legado)
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt formatado
   */
  buildPrompt(question, context, sessionHistory) {
    return this.buildOptimizedPrompt(question, context, sessionHistory);
  }

  /**
   * Verifica se a API OpenAI est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isOpenAIConfigured() {
    return !!config.OPENAI_API_KEY && config.OPENAI_API_KEY !== 'your_openai_api_key_here';
  }

  /**
   * Verifica se a API Gemini est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isGeminiConfigured() {
    return !!config.GEMINI_API_KEY && config.GEMINI_API_KEY !== 'your_gemini_api_key_here';
  }

  /**
   * Verifica se alguma API est√° configurada (m√©todo legado)
   * @returns {boolean} Status da configura√ß√£o
   */
  isConfigured() {
    return this.isOpenAIConfigured() || this.isGeminiConfigured();
  }

  /**
   * Testa a conex√£o com as APIs de IA
   * @returns {Promise<Object>} Status das conex√µes
   */
  async testConnection() {
    const results = {
      openai: false,
      gemini: false,
      anyAvailable: false
    };

    // Teste OpenAI
    if (this.isOpenAIConfigured()) {
      try {
        const openai = this._initializeOpenAI();
        if (openai) {
          const completion = await openai.chat.completions.create({
            model: this.openaiModel,
            messages: [{ role: "user", content: "Teste de conex√£o" }],
            max_tokens: 10,
          });
          results.openai = true;
          console.log('‚úÖ OpenAI: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå OpenAI: Erro no teste de conex√£o:', error.message);
      }
    }

    // Teste Gemini
    if (this.isGeminiConfigured()) {
      try {
        const gemini = this._initializeGemini();
        if (gemini) {
          const model = gemini.getGenerativeModel({ model: this.geminiModel });
          const result = await model.generateContent("Teste de conex√£o");
          results.gemini = true;
          console.log('‚úÖ Gemini: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå Gemini: Erro no teste de conex√£o:', error.message);
      }
    }

    results.anyAvailable = results.openai || results.gemini;
    
    if (!results.anyAvailable) {
      console.warn('‚ö†Ô∏è Nenhuma API de IA dispon√≠vel');
    }

    return results;
  }
}

module.exports = new OpenAIService();
